{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW\n",
    "\n",
    "This R Markdown script uses the **_Boston Housing_** data set to illustrate the following:\n",
    "\n",
    "- The **$k$-Nearest Neighbors** (**KNN**) algorithm;\n",
    "- The **Bias-Variance Trade-Off**; and\n",
    "- The use of **Cross Validation** to estimate Out-of-Sample (OOS) prediction error and determine optimal hyper-parameters, in this case the number of nearest neighbors $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _first, some boring logistics..._\n",
    "\n",
    "Let's first import some necessary Python packages and helper modules from our **_zzz Utility Code_** folder, and set the random number generator's seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enable In-Line MatPlotLib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import:\n",
    "from __future__ import division, print_function\n",
    "from ggplot import aes, ggplot, geom_line, geom_point, ggtitle, scale_color_manual, theme, xlab, ylab\n",
    "from matplotlib import rcParams\n",
    "from numpy import atleast_2d, log, nan, sqrt\n",
    "from pandas import DataFrame, melt, read_csv\n",
    "from random import seed\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sys import path\n",
    "\n",
    "path.append('../../zzz Utility Code/Python')\n",
    "from Helpy import rmse\n",
    "\n",
    "seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing data set\n",
    "\n",
    "Let's now import the Boston Housing data into a **`pandas`** data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read Boston Housing data into data frame\n",
    "boston_housing = read_csv(\n",
    "    'https://raw.githubusercontent.com/ChicagoBoothML/MachineLearning_Fall2015/master/Programming%20Scripts/Boston%20Housing/DATA_BostonHousing.csv')\n",
    "boston_housing.sort(columns='lstat', inplace=True)\n",
    "nb_samples = len(boston_housing)\n",
    "boston_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then focus on the two variables of interest: **`lstat`** (our predictor variable(s) $\\mathbf X$) and **`medv`** (our variable to predict $\\mathbf y$). Below is a plot of them against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x='lstat', y='medv'), data=boston_housing) +\\\n",
    "    geom_point(size=10, color='blue') +\\\n",
    "    ggtitle('Boston Housing: medv vs. lstat') +\\\n",
    "    xlab('lstat') + ylab('medv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-Nearest Neighbors algorithm and Bias-Variance Trade-Off\n",
    "\n",
    "Let's now try fitting a KNN predictor, with $k = 5$, of _medv_ from _lstat_, using all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "knn_model.fit(X=boston_housing[['lstat']], y=boston_housing.medv)\n",
    "boston_housing['predicted_medv'] = knn_model.predict(boston_housing[['lstat']])\n",
    "\n",
    "ggplot(aes(x='lstat', y='medv'), data=boston_housing) +\\\n",
    "    geom_point(size=10, color='blue') +\\\n",
    "    geom_line(aes(x='lstat', y='predicted_medv'), size=2, color='darkorange') +\\\n",
    "    ggtitle('KNN predictor with k = %i' % k) +\\\n",
    "    xlab('lstat') + ylab('medv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $k = 5$ &ndash; a small number of nearest neighbors &ndash; we have a very \"squiggly\" predictor, which **fits the training data well** but is **over-sensitive to small changes** in the _lstat_ variable. We call this a **LOW-BIAS**, **HIGH-VARIANCE** predictor. We don't like it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with, say, $k = 200$, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 200\n",
    "knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "knn_model.fit(X=boston_housing[['lstat']], y=boston_housing.medv)\n",
    "boston_housing['predicted_medv'] = knn_model.predict(boston_housing[['lstat']])\n",
    "\n",
    "ggplot(aes(x='lstat', y='medv'), data=boston_housing) +\\\n",
    "    geom_point(size=10, color='blue') +\\\n",
    "    geom_line(aes(x='lstat', y='predicted_medv'), size=2, color='darkorange') +\\\n",
    "    ggtitle('KNN predictor with k = %i' % k) +\\\n",
    "    xlab('lstat') + ylab('medv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Meh..._, we're not exactly jumping around with joy with this one, either. The predictor line is **not over-sensitive**, but **too \"smooth\" and too simple**, **not responding sufficiently to significant changes** in _lstat_. We call this a **HIGH-BIAS, LOW-VARIANCE** predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something in between, say, $k = 50$, to see if we have any better luck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 50\n",
    "knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "knn_model.fit(X=boston_housing[['lstat']], y=boston_housing.medv)\n",
    "boston_housing['predicted_medv'] = knn_model.predict(boston_housing[['lstat']])\n",
    "\n",
    "ggplot(aes(x='lstat', y='medv'), data=boston_housing) +\\\n",
    "    geom_point(size=10, color='blue') +\\\n",
    "    geom_line(aes(x='lstat', y='predicted_medv'), size=2, color='darkorange') +\\\n",
    "    ggtitle('KNN predictor with k = %i' % k) +\\\n",
    "    xlab('lstat') + ylab('medv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this looks pretty reasonable, and we'd think this predictor would **generalize well** when facing new, not yet seen, data. This is a **low-bias**, **low-variance** predictor. We love ones like this.\n",
    "\n",
    "Hence, the key take-away is that, throughout a range of **hyper-parameter** $k$ from small to large, we have seen a spectrum of corresponding predictors from \"low-bias high-variance\" to \"high-bias low-variance\". This phenomenon is called the **BIAS-VARIANCE TRADE OFF**, a fundamental concept in Machine Learning that is applicable to not only KNN alone but to all modeling methods.\n",
    "\n",
    "The bias-variance trade-off concerns the **generalizability of a trained predictor** in light of new data it's not seen before. If a predictor has high bias and/or high variance, it will not do well in new cases. **Good, generalizable predictors** need to have **both low bias and low variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Sample Error and Cross-Validation\n",
    "\n",
    "To **quantify the generalizability of a predictor**, we need to estimate its **out-of-sample (OOS) error**, i.e. a certain measure of **how well the predictor performs on data not used in its training process**.\n",
    "\n",
    "A popular way to produce such OOS error estimates is to perform **cross validation**. Refer to lecture slides or <a href=\"http://en.wikipedia.org/wiki/Cross-validation_(statistics)\">here</a> for discussions on cross validation.\n",
    "\n",
    "Now, let's consider [**Root Mean Square Error** (**RMSE**)](http://en.wikipedia.org/wiki/Root-mean-square_deviation) as our predictor-goodness evaluation criterion and use **5-fold** cross validation **6 times** to pick a KNN predictor that has satisfactory RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define Root-Mean-Square-Error scoring/evaluation function\n",
    "# compliant with what SciKit Learn expects in this guide:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score\n",
    "def rmse_score(estimator, X, y):\n",
    "    y_hat = estimator.predict(X)\n",
    "    return rmse(y_hat, y)\n",
    "\n",
    "NB_CROSS_VALIDATION_FOLDS = 5\n",
    "NB_CROSS_VALIDATIONS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_range = range(2, 201)\n",
    "cross_validations_avg_rmse_dataframe = DataFrame(dict(k=k_range, model_complexity=-log(k_range)))\n",
    "cross_validations_avg_rmse_dataframe['cv_avg_rmse'] = 0.\n",
    "cv_column_names = []\n",
    "for v in range(NB_CROSS_VALIDATIONS):\n",
    "    cv_column_name = 'cv_%i_rmse' % v\n",
    "    cv_column_names.append(cv_column_name)\n",
    "    cross_validations_avg_rmse_dataframe[cv_column_name] = nan\n",
    "    for k in k_range:\n",
    "        knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "        avg_rmse_score = cross_val_score(\n",
    "            knn_model,\n",
    "            X=boston_housing[['lstat']],\n",
    "            y=boston_housing.medv,\n",
    "            cv=KFold(n=nb_samples,\n",
    "                     n_folds=NB_CROSS_VALIDATION_FOLDS,\n",
    "                     shuffle=True),\n",
    "            scoring=rmse_score).mean()\n",
    "        cross_validations_avg_rmse_dataframe.ix[\n",
    "            cross_validations_avg_rmse_dataframe.k==k, cv_column_name] = avg_rmse_score\n",
    "        \n",
    "    cross_validations_avg_rmse_dataframe.cv_avg_rmse +=\\\n",
    "        (cross_validations_avg_rmse_dataframe[cv_column_name] -\n",
    "         cross_validations_avg_rmse_dataframe.cv_avg_rmse) / (v + 1)\n",
    "        \n",
    "cross_validations_avg_rmse_longdataframe = melt(\n",
    "    cross_validations_avg_rmse_dataframe,\n",
    "    id_vars=['model_complexity', 'cv_avg_rmse'], value_vars=cv_column_names)\n",
    "\n",
    "ggplot(aes(x='model_complexity', y='value', color='variable'),\n",
    "       data=cross_validations_avg_rmse_longdataframe) +\\\n",
    "    geom_line(size=1, linetype='dashed') +\\\n",
    "    geom_line(aes(x='model_complexity', y='cv_avg_rmse'),\n",
    "              data=cross_validations_avg_rmse_longdataframe,\n",
    "              size=2, color='black') +\\\n",
    "    ggtitle('Cross Validations') +\\\n",
    "    xlab('Model Complexity (-log K)') + ylab('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best $k$ that minimizes average cross-validation RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_k_index = cross_validations_avg_rmse_dataframe.cv_avg_rmse.argmin()\n",
    "best_k = k_range[best_k_index]\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = best_k\n",
    "knn_model = KNeighborsRegressor(n_neighbors=k)\n",
    "knn_model.fit(X=boston_housing[['lstat']], y=boston_housing.medv)\n",
    "boston_housing['predicted_medv'] = knn_model.predict(boston_housing[['lstat']])\n",
    "\n",
    "ggplot(aes(x='lstat', y='medv'), data=boston_housing) +\\\n",
    "    geom_point(size=10, color='blue') +\\\n",
    "    geom_line(aes(x='lstat', y='predicted_medv'), size=2, color='darkorange') +\\\n",
    "    ggtitle('KNN predictor with k = %i' % k) +\\\n",
    "    xlab('lstat') + ylab('medv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
